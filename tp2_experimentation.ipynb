{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de0abdcf",
   "metadata": {},
   "source": [
    "# Entrenamiento y evaluación de modelos\n",
    "## Contratación de Seguros de Viajes  \n",
    "  \n",
    "  \n",
    "  \n",
    "#### Preprocesamiento de los datos  \n",
    "Como primera instancia, aplicamos las transformaciones sobre el dataset que indicamos en el trabajo práctico anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf5c98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las dependencias necesarias.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import sklearn_pandas\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn_pandas import DataFrameMapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720e7dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos el dataset.\n",
    "dataset = pd.read_csv('TravelInsurancePrediction.csv')\n",
    "\n",
    "# Renombramos las variables.\n",
    "dataset = dataset.rename(columns={'Idx':'idx', 'Age':'age', 'Employment Type':'employment_type', 'GraduateOrNot':'graduate_or_not', 'AnnualIncome':'annual_income', 'FamilyMembers':'family_members', 'ChronicDiseases':'chronic_diseases', 'EverTravelledAbroad':'ever_travelled_abroad', 'FrequentFlyer':'frequent_flyer', 'TravelInsurance':'travel_insurance'})\n",
    "\n",
    "# Seleccionamos la primer columna 'idx', como índice.\n",
    "dataset = dataset.set_index(\"idx\")\n",
    "\n",
    "# Reemplazamos los valores 'Yes' y 'No' por 0 y 1 de las variables indicadas en el tp1.\n",
    "dataset[\"graduate_or_not\"] = dataset.graduate_or_not.replace(['No', 'Yes'], [0,1])\n",
    "dataset[\"frequent_flyer\"] = dataset.frequent_flyer.replace(['No', 'Yes'], [0,1])\n",
    "dataset[\"ever_travelled_abroad\"] = dataset.ever_travelled_abroad.replace(['No', 'Yes'], [0,1])\n",
    "\n",
    "# Reemplazamos los dos posibles valores de la columna 'employment_type' por 0 y 1.\n",
    "dataset[\"employment_type\"] = dataset.employment_type.replace(['Private Sector/Self Employed', 'Government Sector'], [0,1])\n",
    "\n",
    "# Dividimos el dataset en train (60%), test (20%) y validation (20%)\n",
    "train, not_train = train_test_split(dataset, test_size=0.4, random_state=42)\n",
    "validation, test = train_test_split(not_train, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6951c9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalamos las variables numéricas.\n",
    "mapper = DataFrameMapper([\n",
    "    (['age'],[MinMaxScaler()]),\n",
    "    (['employment_type'],None),\n",
    "    (['graduate_or_not'],None),\n",
    "    (['annual_income'],[MinMaxScaler()]),\n",
    "    (['family_members'],[MinMaxScaler()]),\n",
    "    (['chronic_diseases'],None),\n",
    "    (['frequent_flyer'],None),\n",
    "    (['ever_travelled_abroad'],None),\n",
    "    (['travel_insurance'],None)\n",
    "])\n",
    "\n",
    "mapper.fit(train)\n",
    "preprocessing_ds = pd.DataFrame(mapper.transform(dataset), columns=['age', 'employment_type', 'graduate_or_not', 'annual_income', 'family_members', 'chronic_diseases', 'frquent_flyer', 'ever_travelled_abroad', 'travel_insurance'])\n",
    "preprocessing_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84eab37b",
   "metadata": {},
   "source": [
    "#### Elección de una métrica  \n",
    "Teniendo en cuenta que el objetivo de implementar un modelo para este dataset podría ser detectar aquellas personas que no van a contratar el seguro, y así idear estrategias de marketing para intentar persuadirlas de lo contrario, decidimos utilizar la métrica Precission. Como nos interesa enfocarnos en las personas que no adquieren el seguro, deberíamos poder detectar los falsos positivos, es decir, aquellos clientes clasificados como que sí compran, pero en realidad no lo hacen. Y justamente elegimos Precission por la forma en que opera, ya que nos da la posibilidad de enfocarnos en esto.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09651711",
   "metadata": {},
   "source": [
    "#### Elección de una técnica de feature engineering  \n",
    "Teniendo en cuenta las técnicas vistas en clase y el formato de los datos que tenemos en nuestro dataset, decidimos que la mejor opción sería aplicar Binning (redondeo de números) sobre las variables numéricas para las que aplicamos MaxMinScaler (age, annual_income, family_members) ya que al hacer esto quedaron valores con una gran cantidad de decimales. La técnica de Binning lo que haría sería redondear o convertir a rangos fijos los valores, sacando precisión de los mismo. Esto podría llegar a servir para evitar el sobreentrenamiento y que el modelo generalice mejor. En el caso de la columna 'age' se dio que ningún ejemplo posee un valor con gran cantidad de decimales, pero de todas formas utilizamos la técnica sobre esta, previendo que en un futuro al utilizar nuevas observaciones no suceda esta misma situación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760c1a76",
   "metadata": {},
   "source": [
    "#### Entrenamiento y evaluación de modelos  \n",
    "Para este dataset seleccionamos los siguientes modelos:\n",
    "* k-Nearest Neighbors o k-NNN (vecinos cercanos)\n",
    "* Decision Tree (árbol de decisión)\n",
    "* Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a9ad4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para evaluar modelos.\n",
    "def evaluate_model(model, set_names=('train', 'validation'), title='', show_cm=True):\n",
    "    if title:\n",
    "        display(title)\n",
    "        \n",
    "    final_metrics = defaultdict(list)\n",
    "    \n",
    "    if show_cm:\n",
    "        fig, axis = plt.subplots(1, len(set_names), sharey=True, figsize=(15, 3))\n",
    "    \n",
    "    for i, set_name in enumerate(set_names):\n",
    "        assert set_name in ['train', 'validation', 'test']\n",
    "        set_data = globals()[set_name] \n",
    "\n",
    "        y = set_data.travel_insurance\n",
    "        y_pred = model.predict(set_data)\n",
    "        final_metrics['Accuracy'].append(metrics.accuracy_score(y, y_pred))\n",
    "        final_metrics['Precision'].append(metrics.precision_score(y, y_pred))\n",
    "        \n",
    "        if show_cm:\n",
    "            ax = axis[i]\n",
    "            sns.heatmap(metrics.confusion_matrix(y, y_pred), ax=ax, cmap='Blues', annot=True, fmt='.0f', cbar=False)\n",
    "\n",
    "            ax.set_title(set_name)\n",
    "            ax.xaxis.set_ticklabels(['No', 'Yes'])\n",
    "            ax.yaxis.set_ticklabels(['No', 'Yes'])\n",
    "            ax.set_xlabel('Predicted class (travel_insurance)')\n",
    "            ax.set_ylabel('True class (travel_insurance)')\n",
    "\n",
    "        \n",
    "    display(pd.DataFrame(final_metrics, index=set_names))\n",
    "    if show_cm:\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
